# /bootstrap - Generate Data Pipeline Specifications from Code

Generate initial specification structure for data pipelines by analyzing existing ETL code, data flows, and transformations.

## Spec Types to Generate

### sources/
Data sources and ingestion:
- Database connections and queries
- API endpoints and webhooks
- File systems and object storage
- Streaming systems (Kafka, Kinesis, etc.)
- Data extraction patterns

### transforms/
Data transformations and processing:
- Data cleaning and normalization
- Business logic and calculations
- Aggregations and joins
- Data enrichment
- Quality checks and validations

### sinks/
Output destinations:
- Data warehouses (Snowflake, BigQuery, Redshift)
- Data lakes (S3, ADLS, GCS)
- Databases and caches
- Analytics platforms
- Downstream APIs

### schemas/
Data schemas and contracts:
- Input data schemas
- Transformation schemas
- Output schemas
- Schema evolution rules
- Data validation rules

### orchestration/
Workflow and scheduling:
- DAG definitions and dependencies
- Scheduling policies
- Retry and error handling
- Monitoring and alerting
- Resource management

[Placeholder - Full command content to be developed]
